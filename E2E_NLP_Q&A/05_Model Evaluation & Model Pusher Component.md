1.
What is model evaluation in machine learning?

2.
Explain the difference between bias and variance in the context of model evaluation.

3.
What are the commonly used metrics for evaluating classification models?

4.
Discuss the importance of cross-validation in model evaluation.

5.
What is overfitting and underfitting in the context of model evaluation?

6.
How can you prevent overfitting in a machine learning model?

7.
What is precision and recall in the context of model evaluation?

8.
Explain the F1 score and why it is a better metric than accuracy in some cases.

9.
What is the ROC curve and how is it used to evaluate model performance?

10.
Define confusion matrix. How is it useful in model evaluation?



11.
Discuss the concept of model pusher component in a machine learning pipeline.

12.
What are the key considerations when deploying a machine learning model using a model pusher component?

13.
Explain the role of monitoring and scaling in the model pusher component.



14.
How can automated continuous evaluation of models be integrated into a model pusher component?


15.
Compare and contrast batch model deployment versus real-time model deployment using the model pusher component.














































































